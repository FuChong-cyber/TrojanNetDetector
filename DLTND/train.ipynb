{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Trains a model, saving checkpoints and tensorboard summaries along\n",
    "   the way.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tqdm import trange\n",
    "\n",
    "import dataset_input\n",
    "from eval import evaluate \n",
    "import resnet_train\n",
    "import utilities\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "def train(config):\n",
    "    # seeding randomness\n",
    "    tf.set_random_seed(config.training.tf_random_seed)\n",
    "    np.random.seed(config.training.np_random_seed)\n",
    "\n",
    "    # Setting up training parameters\n",
    "    max_num_training_steps = config.training.max_num_training_steps\n",
    "    step_size_schedule = config.training.step_size_schedule\n",
    "    weight_decay = config.training.weight_decay\n",
    "    momentum = config.training.momentum\n",
    "    batch_size = config.training.batch_size\n",
    "    eval_during_training = config.training.eval_during_training\n",
    "    num_clean_examples = config.training.num_examples\n",
    "    if eval_during_training:\n",
    "        num_eval_steps = config.training.num_eval_steps\n",
    "\n",
    "    # Setting up output parameters\n",
    "    num_output_steps = config.training.num_output_steps\n",
    "    num_summary_steps = config.training.num_summary_steps\n",
    "    num_checkpoint_steps = config.training.num_checkpoint_steps\n",
    "\n",
    "    # Setting up the data and the model\n",
    "    dataset = dataset_input.CIFAR10Data(config,\n",
    "                                            seed=config.training.np_random_seed)\n",
    "    print('Num Poisoned Left: {}'.format(dataset.num_poisoned_left))\n",
    "    print('Poison Position: {}'.format(config.data.position))\n",
    "    print('Poison Color: {}'.format(config.data.color))\n",
    "    num_training_examples = len(dataset.train_data.xs)\n",
    "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "    model = resnet_train.Model(config.model)\n",
    "\n",
    "    # uncomment to get a list of trainable variables\n",
    "    model_vars = tf.trainable_variables()\n",
    "    slim.model_analyzer.analyze_vars(model_vars, print_info=True)\n",
    "\n",
    "    # Setting up the optimizer\n",
    "    boundaries = [int(sss[0]) for sss in step_size_schedule]\n",
    "    boundaries = boundaries[1:]\n",
    "    values = [sss[1] for sss in step_size_schedule]\n",
    "    learning_rate = tf.train.piecewise_constant(\n",
    "        tf.cast(global_step, tf.int32),\n",
    "        boundaries,\n",
    "        values)\n",
    "    total_loss = model.mean_xent + weight_decay * model.weight_decay_loss\n",
    "\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    train_step = optimizer.minimize( total_loss, global_step=global_step)\n",
    "\n",
    "    # Setting up the Tensorboard and checkpoint outputs\n",
    "    model_dir = config.model.output_dir\n",
    "    if eval_during_training:\n",
    "        eval_dir = os.path.join(model_dir, 'eval')\n",
    "        if not os.path.exists(eval_dir):\n",
    "            os.makedirs(eval_dir)\n",
    "\n",
    "    # We add accuracy and xent twice so we can easily make three types of\n",
    "    # comparisons in Tensorboard:\n",
    "    # - train vs eval (for a single run)\n",
    "    # - train of different runs\n",
    "    # - eval of different runs\n",
    "\n",
    "    saver = tf.train.Saver(max_to_keep=3)\n",
    "\n",
    "    tf.summary.scalar('accuracy_nat_train', model.accuracy, collections=['nat'])\n",
    "    tf.summary.scalar('accuracy_nat', model.accuracy, collections = ['nat'])\n",
    "    tf.summary.scalar('xent_nat_train', model.xent / batch_size,\n",
    "                                                        collections=['nat'])\n",
    "    tf.summary.scalar('xent_nat', model.xent / batch_size, collections=['nat'])\n",
    "    tf.summary.image('images_nat_train', model.train_xs, collections=['nat'])\n",
    "    tf.summary.scalar('learning_rate', learning_rate, collections=['nat'])\n",
    "    nat_summaries = tf.summary.merge_all('nat')\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        print('Dataset Size: ', len(dataset.train_data.xs))\n",
    "\n",
    "          # Initialize the summary writer, global variables, and our time counter.\n",
    "        summary_writer = tf.summary.FileWriter(model_dir, sess.graph)\n",
    "        if eval_during_training:\n",
    "            eval_summary_writer = tf.summary.FileWriter(eval_dir)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        training_time = 0.0\n",
    "\n",
    "      # Main training loop\n",
    "        for ii in range(max_num_training_steps+1):\n",
    "            x_batch, y_batch = dataset.train_data.get_next_batch(batch_size,\n",
    "                                                           multiple_passes=True)\n",
    "\n",
    "            nat_dict = {model.x_input: x_batch,\n",
    "                        model.y_input: y_batch,\n",
    "                        model.is_training: False}\n",
    "\n",
    "        # Output to stdout\n",
    "            if ii % num_output_steps == 0:\n",
    "                nat_acc = sess.run(model.accuracy, feed_dict=nat_dict)\n",
    "                print('Step {}:    ({})'.format(ii, datetime.now()))\n",
    "                print('    training nat accuracy {:.4}%'.format(nat_acc * 100))\n",
    "                if ii != 0:\n",
    "                    print('    {} examples per second'.format(\n",
    "                        num_output_steps * batch_size / training_time))\n",
    "                    training_time = 0.0\n",
    "\n",
    "        # Tensorboard summaries\n",
    "            if ii % num_summary_steps == 0:\n",
    "                summary = sess.run(nat_summaries, feed_dict=nat_dict)\n",
    "                summary_writer.add_summary(summary, global_step.eval(sess))\n",
    "\n",
    "        # Write a checkpoint\n",
    "            if ii % num_checkpoint_steps == 0:\n",
    "                saver.save(sess,\n",
    "                          os.path.join(model_dir, 'checkpoint'),\n",
    "                          global_step=global_step)\n",
    "\n",
    "            if eval_during_training and ii % num_eval_steps == 0:  \n",
    "                evaluate(model, sess, config, eval_summary_writer)\n",
    "\n",
    "        # Actual training step\n",
    "            start = timer()\n",
    "            nat_dict[model.is_training] = True\n",
    "            sess.run(train_step, feed_dict=nat_dict)\n",
    "            end = timer()\n",
    "            training_time += end - start\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config_dict = utilities.get_config('config.json')\n",
    "\n",
    "model_dir = config_dict['model']['output_dir']\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# keep the configuration file with the model for reproducibility\n",
    "with open(os.path.join(model_dir, 'config.json'), 'w') as f:\n",
    "    json.dump(config_dict, f, sort_keys=True, indent=4)\n",
    "\n",
    "# config.data.position = [np.round(np.random.uniform(0,32)),round(np.random.uniform(0,32))]\n",
    "# config.data.color = [round(np.random.uniform(0,255)),round(np.random.uniform(0,255)),round(np.random.uniform(0,255))]\n",
    "config = utilities.config_to_namedtuple(config_dict)\n",
    "\n",
    "train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!conda install --yes tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#78.58% 74.39%, 82.96% 90%, 81.20% 80.62%, 79.04% 94.34%, 79.11% 81.43%, 77.27% 82.77%, 83.68% 90%, 84.25% 70.72%, 84.17% 86.38%, 80.72% 78.47%\n",
    "\n",
    "#78.05% 64.10%, 82.52% 68.90%, 76.45% 65.22%, 78.09% 68.00%, 74% 80%, 77.31% 74.23%, 83.76% 80%, 81.58% 70%, 75.53% 70%, 74.30% 69%\n",
    "\n",
    "#85.00% 39.62%, 82.88% 47.20%, 75.70% 30.86%, 83.67% 33.92%, 82.55% 37.38%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
